# Exsa-Engine: Complete Tech Stack & Architecture

**Version**: 0.1.0  
**Last Updated**: November 24, 2025  
**Status**: Production-Ready Beast Mode ğŸ¦

---

## Table of Contents

1. [Overview](#overview)
2. [Core Technology Stack](#core-technology-stack)
3. [System Architecture](#system-architecture)
4. [Component Breakdown](#component-breakdown)
5. [Dependencies](#dependencies)
6. [Hardware Integration](#hardware-integration)
7. [Performance Optimizations](#performance-optimizations)
8. [API Specification](#api-specification)
9. [File Structure](#file-structure)
10. [Design Patterns](#design-patterns)

---

## Overview

**Exsa-Engine** is a high-performance, GPU-accelerated LLM inference engine built in Rust, designed for edge AI deployment with universal GGUF model support.

### Key Metrics
- **Performance**: 37-61 tokens/second (avg ~51 t/s)
- **Startup Time**: 0.77-0.9 seconds
- **GPU Utilization**: 100% (Metal/CUDA/ROCm)
- **Model Support**: Universal GGUF format
- **Codebase**: ~2000+ lines of Rust

---

## Core Technology Stack

### Programming Language
```
Rust 2021 Edition
â”œâ”€ Version: 1.70+ (stable)
â”œâ”€ Features: async/await, procedural macros
â””â”€ Toolchain: stable-aarch64-apple-darwin
```

### Runtime Environment
```
Tokio Async Runtime
â”œâ”€ Version: 1.35
â”œâ”€ Features: Multi-threaded scheduler
â””â”€ Purpose: Async I/O, HTTP server
```

### Build System
```
Cargo
â”œâ”€ Build: Release profile (opt-level = 3)
â”œâ”€ Features: Conditional compilation (metal/cuda/rocm)
â””â”€ Platform: Universal (macOS/Linux/Windows)
```

---

## System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Exsa-Engine                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  HTTP Server â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤   REST API   â”‚            â”‚
â”‚  â”‚   (Axum)     â”‚         â”‚  (OpenAI)    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚                                              â”‚
â”‚         â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚     Inference Engine Core            â”‚             â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚
â”‚  â”‚  â”‚  Standard  â”‚  â”‚  Speculative   â”‚ â”‚             â”‚
â”‚  â”‚  â”‚ Generation â”‚  â”‚   Decoding     â”‚ â”‚             â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚
â”‚  â”‚  â”‚   Batch    â”‚  â”‚   KV Cache     â”‚ â”‚             â”‚
â”‚  â”‚  â”‚  Manager   â”‚  â”‚     Pool       â”‚ â”‚             â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                â”‚                                       â”‚
â”‚                â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚      llama.cpp Backend             â”‚               â”‚
â”‚  â”‚   (Rust Bindings: llama-cpp-2)    â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                â”‚                                       â”‚
â”‚                â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚      GPU Acceleration Layer        â”‚               â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚               â”‚
â”‚  â”‚  â”‚ Metal  â”‚ â”‚ CUDA â”‚ â”‚  ROCm   â”‚  â”‚               â”‚
â”‚  â”‚  â”‚ (macOS)â”‚ â”‚(NVIDIA)â”‚  (AMD)  â”‚  â”‚               â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request Flow Diagram

```
User Request
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HTTP Endpoint   â”‚ (Axum Router)
â”‚ /v1/generate    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request Queue   â”‚ (mpsc channel)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch Manager   â”‚ (Optional: Continuous batching)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inference Engineâ”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Speculativeâ”‚  â”‚ (Optional: If enabled)
â”‚  â”‚  Engine   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ llama.cpp Core  â”‚
â”‚  - Tokenize     â”‚
â”‚  - Forward Pass â”‚
â”‚  - Sampling     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPU/CPU Exec   â”‚ (Metal/CUDA/ROCm/CPU)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token Stream    â”‚ (Server-Sent Events)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    User Response
```

### Data Flow

```
Model File (GGUF)
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Loader   â”‚
â”‚  - Validation   â”‚
â”‚  - mmap         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPU Offload     â”‚
â”‚ (Metal/CUDA)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inference Ready â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Prompt
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Tokenizer     â”‚ (llama.cpp)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token IDs      â”‚ [101, 2054, 2003...]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Forward   â”‚ (GPU accelerated)
â”‚    Pass         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Logits        â”‚ [0.1, 0.5, 0.3...]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Sampler      â”‚ (temp, top_k, top_p)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Next Token ID  â”‚ â†’ Stream to user
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Breakdown

### 1. HTTP Server Layer

**Technology**: Axum Web Framework
```rust
Dependencies:
- axum = "0.7"
- tower = "0.4"
- tower-http = "0.5"
```

**Responsibilities**:
- RESTful API endpoints
- Request routing
- CORS handling
- Server-Sent Events (SSE) streaming
- Error handling

**Endpoints**:
```
GET  /v1/health      - Health check
POST /v1/generate    - Text generation
POST /v1/chat        - Chat completion (planned)
GET  /v1/models      - List models (planned)
```

### 2. Inference Engine Core

**Location**: `src/inference/engine.rs`

**Components**:
- **Standard Generation**: Basic token-by-token generation
- **Speculative Decoding**: Draft + verify for 2-3x speedup
- **Batch Manager**: Continuous batching for concurrent requests
- **KV Cache Pool**: Efficient memory management

**Key Features**:
```rust
- Async/await architecture
- Lock-free where possible
- Zero-copy token streaming
- Configurable sampling parameters
```

### 3. Model Management

**Location**: `src/model/`

**Modules**:
```
model/
â”œâ”€â”€ loader.rs    - GGUF file loading & validation
â”œâ”€â”€ config.rs    - Model configuration
â””â”€â”€ mod.rs       - Public API
```

**Capabilities**:
- GGUF format validation
- Memory-mapped file I/O (mmap)
- GPU layer offloading
- Model metadata extraction

### 4. Speculative Decoding Engine

**Location**: `src/inference/speculative.rs`

**Architecture**: Dual-model system
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Draft Model  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Target Model â”‚
â”‚  (Small)     â”‚ Verify  â”‚   (Main)     â”‚
â”‚  Fast        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Accurate   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Algorithm**:
1. Draft model generates K tokens (fast)
2. Target model verifies in parallel (batch)
3. Accept verified tokens, reject others
4. Repeat

**Expected Speedup**: 2-3x on compatible prompts

### 5. Continuous Batching System

**Location**: `src/inference/batch_manager.rs`

**Components**:
- Request queue (FIFO/Priority)
- Batch assembler
- KV cache allocator
- Scheduling strategies

**Strategies**:
```rust
pub enum SchedulingStrategy {
    FIFO,               // Fair
    ShortestFirst,      // Minimize latency
    Priority,           // User-defined
    Dynamic,            // Adaptive
}
```

**Benefits**:
- 3-5x throughput for concurrent requests
- Efficient GPU utilization
- Reduced latency variance

### 6. KV Cache Management

**Location**: `src/inference/kv_cache.rs`

**Architecture**: Object pool pattern
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       KV Cache Pool          â”‚
â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”       â”‚
â”‚  â”‚ C1 â”‚ â”‚ C2 â”‚ â”‚ C3 â”‚ ...   â”‚
â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜       â”‚
â”‚  Available: 13 / 16          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚           â”‚
        â–¼           â–¼
   Request 1   Request 2
```

**Features**:
- Pre-allocated cache slots
- Lock-free acquisition
- Automatic cleanup
- Configurable pool size

---

## Dependencies

### Core Dependencies

```toml
[dependencies]
# LLM Inference
llama-cpp-2 = { version = "0.1.126", features = ["metal"] }

# Async Runtime
tokio = { version = "1.35", features = ["full"] }

# HTTP Server
axum = "0.7"
tower = "0.4"
tower-http = { version = "0.5", features = ["cors", "trace"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# Utilities
thiserror = "1.0"
anyhow = "1.0"
uuid = { version = "1.6", features = ["v4", "serde"] }

# Environment
dotenv = "0.15"

# Memory mapping
memmap2 = "0.9"
```

### Feature Flags

```toml
[features]
default = []
metal = ["llama-cpp-2/metal"]    # macOS GPU
cuda = ["llama-cpp-2/cuda"]      # NVIDIA GPU
rocm = []                        # AMD GPU (future)
cpu = []                         # CPU-only mode
```

### Build Dependencies

```toml
[build-dependencies]
# Currently none - may add for custom build scripts
```

---

## Hardware Integration

### GPU Acceleration

#### Metal (macOS)
```
Technology: Apple Metal API
Supported: M1, M2, M3, M4 processors
Integration: llama-cpp-2 Metal backend
Performance: 100% GPU offload, ~51 t/s
```

#### CUDA (NVIDIA)
```
Technology: NVIDIA CUDA Toolkit
Supported: RTX 20xx, 30xx, 40xx series
Integration: llama-cpp-2 CUDA backend
Performance: Expected ~60-80 t/s (depends on GPU)
```

#### ROCm (AMD)
```
Technology: AMD ROCm platform
Supported: RX 6000, 7000 series
Integration: Planned (llama.cpp support)
Performance: Expected ~50-70 t/s
```

#### CPU Fallback
```
Technology: Native CPU SIMD
Supported: x86_64 (AVX2), ARM64 (NEON)
Integration: llama-cpp-2 CPU backend
Performance: ~10-20 t/s (varies by CPU)
```

### Memory Architecture

```
System RAM
    â”‚
    â”œâ”€ Model File (mmap)
    â”‚     â””â”€ 1.5 GB (LFM2-2.6B Q4)
    â”‚
    â”œâ”€ KV Cache
    â”‚     â””â”€ ~500 MB (context 2048)
    â”‚
    â””â”€ Application Memory
          â””â”€ ~50 MB

GPU VRAM (if available)
    â”‚
    â”œâ”€ Model Weights
    â”‚     â””â”€ Offloaded layers
    â”‚
    â””â”€ Compute Buffers
          â””â”€ Temporary activations
```

---

## Performance Optimizations

### Compiler Optimizations

```rust
[profile.release]
opt-level = 3              # Maximum optimization
lto = true                 # Link-time optimization
codegen-units = 1          # Single codegen unit
strip = true               # Strip symbols
```

### Runtime Optimizations

1. **Memory-Mapped I/O (mmap)**
   - Zero-copy model loading
   - OS-level caching
   - Lazy loading

2. **Batch Processing**
   - Optimized batch size: 256
   - GPU-friendly batching
   - Reduced kernel launches

3. **Async Architecture**
   - Non-blocking I/O
   - Concurrent request handling
   - Lock-free data structures

4. **GPU Offloading**
   - 100% layer offload
   - Optimized memory transfers
   - Kernel fusion

### Algorithmic Optimizations

1. **Speculative Decoding**
   - 2-3x theoretical speedup
   - Parallel verification
   - Adaptive speculation depth

2. **Continuous Batching**
   - 3-5x throughput gain
   - Dynamic batch assembly
   - Smart scheduling

3. **KV Cache Pooling**
   - Pre-allocation
   - Zero-allocation serving
   - Efficient reuse

---

## API Specification

### OpenAI-Compatible API

#### Generate Endpoint

```http
POST /v1/generate
Content-Type: application/json

{
  "prompt": "string",
  "sampling_params": {
    "temperature": 0.7,
    "max_tokens": 100,
    "top_k": 40,
    "top_p": 0.9,
    "repeat_penalty": 1.1,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0,
    "stop_sequences": [],
    "seed": 42,
    "min_p": 0.05,
    "mirostat": 0,
    "mirostat_tau": 5.0,
    "mirostat_eta": 0.1,
    "repeat_last_n": 64,
    "tfs_z": 1.0,
    "typical_p": 1.0
  }
}
```

**Response** (Server-Sent Events):
```
data: {"token": "Hello", "done": false}

data: {"token": " world", "done": false}

data: {"token": "", "done": true}
```

#### Health Endpoint

```http
GET /v1/health

Response:
{
  "status": "healthy",
  "version": "0.1.0"
}
```

---

## File Structure

```
exsa-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs                 # Entry point
â”‚   â”œâ”€â”€ server/
â”‚   â”‚   â”œâ”€â”€ mod.rs             # HTTP server setup
â”‚   â”‚   â”œâ”€â”€ routes.rs          # API endpoints
â”‚   â”‚   â””â”€â”€ handlers.rs        # Request handlers
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ mod.rs             # Module exports
â”‚   â”‚   â”œâ”€â”€ engine.rs          # Core inference engine
â”‚   â”‚   â”œâ”€â”€ speculative.rs     # Speculative decoding
â”‚   â”‚   â”œâ”€â”€ batch_manager.rs   # Continuous batching
â”‚   â”‚   â”œâ”€â”€ kv_cache.rs        # KV cache pool
â”‚   â”‚   â””â”€â”€ queue.rs           # Request queue
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ mod.rs             # Module exports
â”‚   â”‚   â”œâ”€â”€ loader.rs          # Model loading
â”‚   â”‚   â””â”€â”€ config.rs          # Configuration
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ mod.rs             # Module exports
â”‚       â””â”€â”€ error.rs           # Error types
â”œâ”€â”€ models/                     # Model files (not in repo)
â”‚   â””â”€â”€ LFM2-2.6B-Q4_K_M.gguf
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ adocs/                  # Archived docs
â”œâ”€â”€ tests/                      # Integration tests
â”œâ”€â”€ Cargo.toml                  # Dependencies
â”œâ”€â”€ Cargo.lock                  # Locked versions
â”œâ”€â”€ build.sh                    # Universal build script
â”œâ”€â”€ .env.example                # Example environment vars
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## Design Patterns

### 1. Dependency Injection

```rust
pub struct InferenceEngine {
    model: Arc<LlamaModel>,
    backend: Arc<LlamaBackend>,
    config: ModelConfig,
}
```

**Benefits**: Testability, flexibility

### 2. Builder Pattern

```rust
ModelConfig::new()
    .with_context(2048)
    .with_batch_size(256)
    .with_gpu_layers(99)
    .build()
```

**Benefits**: Ergonomic configuration

### 3. Object Pool Pattern

```rust
pub struct KVCachePool {
    available: Vec<KVCacheSlot>,
    in_use: HashMap<Uuid, KVCacheSlot>,
}
```

**Benefits**: Efficient memory reuse

### 4. Strategy Pattern

```rust
pub enum SchedulingStrategy {
    FIFO,
    ShortestFirst,
    Priority,
    Dynamic,
}
```

**Benefits**: Pluggable algorithms

### 5. Actor Model (via Tokio)

```rust
let (tx, rx) = mpsc::channel(100);
tokio::spawn(async move {
    while let Some(req) = rx.recv().await {
        process(req).await;
    }
});
```

**Benefits**: Concurrency, isolation

### 6. Error Handling: Result + Custom Types

```rust
pub enum ExsaError {
    ModelLoad(String),
    Inference(String),
    ResourceExhausted,
}

pub type Result<T> = std::result::Result<T, ExsaError>;
```

**Benefits**: Type-safe error handling

---

## Configuration

### Environment Variables

```bash
# Model
MODEL_PATH="models/LFM2-2.6B-Q4_K_M.gguf"

# Performance
GPU_LAYERS=99           # Number of layers to offload
CONTEXT_SIZE=2048       # Context window size
BATCH_SIZE=256          # Batch size for processing
CPU_THREADS=11          # CPU threads (if not full GPU)

# Server
HOST="127.0.0.1"
PORT=3000

# Features (Optional)
ENABLE_SPECULATIVE=false
DRAFT_MODEL_PATH=""
SPECULATION_DEPTH=4

ENABLE_CONTINUOUS_BATCHING=false
MAX_BATCH_SIZE=8
BATCH_TIMEOUT_MS=100
KV_CACHE_POOL_SIZE=16
SCHEDULING_STRATEGY="FIFO"
```

---

## Build System

### Universal Build Script

```bash
#!/bin/bash
# build.sh - Auto-detects hardware and builds optimized binary

# Detect platform
if [[ "$OSTYPE" == "darwin"* ]]; then
    # macOS - use Metal
    cargo build --release --features metal
elif command -v nvidia-smi &> /dev/null; then
    # NVIDIA GPU - use CUDA
    cargo build --release --features cuda
elif command -v rocm-smi &> /dev/null; then
    # AMD GPU - use ROCm
    cargo build --release --features rocm
else
    # CPU only
    cargo build --release --features cpu
fi
```

**Usage**:
```bash
chmod +x build.sh
./build.sh
```

---

## Testing Architecture

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_model_loading() {
        // Test model validation
    }

    #[tokio::test]
    async fn test_generation() {
        // Test basic generation
    }
}
```

### Integration Tests

```
tests/
â”œâ”€â”€ integration_test.rs    # End-to-end API tests
â””â”€â”€ performance_test.rs    # Benchmark tests
```

---

## Deployment Options

### 1. Standalone Binary

```bash
./exsa-engine
```

**Use Case**: Local development, single-user

### 2. Systemd Service (Linux)

```ini
[Unit]
Description=Exsa-Engine LLM Inference Server

[Service]
ExecStart=/usr/local/bin/exsa-engine
Restart=always
Environment="MODEL_PATH=/models/model.gguf"

[Install]
WantedBy=multi-user.target
```

### 3. Docker (Future)

```dockerfile
FROM rust:1.70 as builder
WORKDIR /app
COPY . .
RUN cargo build --release

FROM debian:bookworm-slim
COPY --from=builder /app/target/release/exsa-engine /
CMD ["/exsa-engine"]
```

### 4. Cloud Deployment

- AWS EC2 (GPU instances)
- Google Cloud (with GPUs)
- Azure (with GPUs)

---

## Performance Characteristics

### Benchmarks

**Hardware**: Apple M3 Pro (Metal)
**Model**: LFM2-2.6B Q4_K_M

```
Metric                  Value
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Startup Time            0.77-0.9s
Model Load              0.8s
Generation Speed        37-61 t/s
Average Speed           ~51 t/s
GPU Utilization         100%
Memory Usage            ~2 GB
Binary Size             5.4 MB
```

### Scalability

```
Concurrent Requests     Throughput
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1 request               51 t/s
4 requests              45 t/s each (180 t/s total)*
8 requests              40 t/s each (320 t/s total)*

* With continuous batching enabled
```

---

## Security Considerations

### 1. Input Validation
- Prompt size limits
- Parameter bounds checking
- Sanitized error messages

### 2. Network Security
- localhost-only binding by default
- Optional CORS configuration
- Rate limiting (planned)

### 3. Resource Limits
- Max context size
- Max batch size
- Memory pool limits

---

## Future Enhancements

### Planned Features

1. **CLI Interface**
   - Interactive chat mode
   - One-shot generation
   - Model management

2. **Multi-Model Support**
   - Dynamic model switching
   - Model warmup/preloading
   - Model registry

3. **Advanced Features**
   - Quantization on-the-fly
   - LoRA adapter support
   - Custom sampling algorithms

4. **Monitoring**
   - Prometheus metrics
   - Performance dashboards
   - Health checks

---

## Summary

**Exsa-Engine** is a modern, high-performance LLM inference engine built with:

- âœ… **Rust** for safety and performance
- âœ… **llama.cpp** for proven inference
- âœ… **Metal/CUDA/ROCm** for GPU acceleration
- âœ… **Async/await** for concurrency
- âœ… **OpenAI-compatible** API
- âœ… **Production-ready** architecture

**Performance**: 51 t/s average, 0.8s startup, 100% GPU utilization

**Status**: Beast Mode ON ğŸ¦ğŸ”¥

---

*Generated: November 24, 2025*  
*Version: 0.1.0*
